{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“.venv (Python 3.11.11)”的单元格需要ipykernel包。\n",
      "\u001b[1;31m运行以下命令，将 \"ipykernel\" 安装到 Python 环境中。\n",
      "\u001b[1;31m命令: \"/storage/qiguojunLab/caojinjin/codes/openpi/.venv/bin/python -m pip install ipykernel -U --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "import os \n",
    "import sys\n",
    "\n",
    "\n",
    "root_dir = '/storage/qiguojunLab/caojinjin/codes/openpi/src'\n",
    "\n",
    "# 将根目录添加到 sys.path\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "    \n",
    "from openpi.models import model as _model\n",
    "from openpi.policies import droid_policy\n",
    "from openpi.policies import policy_config as _policy_config\n",
    "from openpi.shared import download\n",
    "from openpi.training import config as _config\n",
    "from openpi.training import data_loader as _data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy inference\n",
    "\n",
    "The following example shows how to create a policy from a checkpoint and run inference on a dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: time_horizon, action_dim, min_token, scale, vocab_size. \n",
      "Some kwargs in processor config are unused and will not have any effect: time_horizon, action_dim, min_token, scale, vocab_size. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (10, 8)\n"
     ]
    }
   ],
   "source": [
    "config = _config.get_config(\"pi0_fast_droid\")\n",
    "checkpoint_dir = download.maybe_download(\"s3://openpi-assets/checkpoints/pi0_fast_droid\")\n",
    "\n",
    "# Create a trained policy.\n",
    "policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
    "\n",
    "# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\n",
    "example = droid_policy.make_droid_example()\n",
    "result = policy.infer(example)\n",
    "\n",
    "# Delete the policy to free up memory.\n",
    "del policy\n",
    "\n",
    "print(\"Actions shape:\", result[\"actions\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to visualize the result trajectory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a live model\n",
    "\n",
    "\n",
    "The following example shows how to create a live model from a checkpoint and compute training loss. First, we are going to demonstrate how to do it with fake data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "config = _config.get_config(\"pi0_aloha_sim\")\n",
    "\n",
    "checkpoint_dir = download.maybe_download(\"s3://openpi-assets/checkpoints/pi0_aloha_sim\")\n",
    "key = jax.random.key(0)\n",
    "\n",
    "# Create a model from the checkpoint.\n",
    "model = config.model.load(_model.restore_params(checkpoint_dir / \"params\"))\n",
    "\n",
    "# We can create fake observations and actions to test the model.\n",
    "obs, act = config.model.fake_obs(), config.model.fake_act()\n",
    "\n",
    "# Sample actions from the model.\n",
    "loss = model.compute_loss(key, obs, act)\n",
    "print(\"Loss shape:\", loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create a data loader and use a real batch of training data to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `/storage/qiguojunLab/qiguojun/.cache/huggingface/lerobot/lerobot/aloha_sim_transfer_cube_human` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "WARNING:huggingface_hub._snapshot_download:Returning existing local_dir `/storage/qiguojunLab/qiguojun/.cache/huggingface/lerobot/lerobot/aloha_sim_transfer_cube_human` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage/qiguojunLab/qiguojun/.cache/huggingface/lerobot/lerobot/aloha_sim_transfer_cube_human/meta/info.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(config, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load a single batch of data. This is the same data that will be used during training.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# NOTE: In order to make this example self-contained, we are skipping the normalization step\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# since it requires the normalization statistics to be generated using `compute_norm_stats`.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43m_data_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_norm_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m obs, act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Sample actions from the model.\u001b[39;00m\n",
      "File \u001b[0;32m/storage/qiguojunLab/caojinjin/codes/openpi/src/openpi/training/data_loader.py:155\u001b[0m, in \u001b[0;36mcreate_data_loader\u001b[0;34m(config, sharding, skip_norm_stats, shuffle, num_batches, num_workers)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a data loader for training.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m        execute in the main process.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    153\u001b[0m data_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcreate(config\u001b[38;5;241m.\u001b[39massets_dirs, config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 155\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m dataset \u001b[38;5;241m=\u001b[39m transform_dataset(dataset, data_config, skip_norm_stats\u001b[38;5;241m=\u001b[39mskip_norm_stats)\n\u001b[1;32m    158\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m TorchDataLoader(\n\u001b[1;32m    159\u001b[0m     dataset,\n\u001b[1;32m    160\u001b[0m     local_batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m jax\u001b[38;5;241m.\u001b[39mprocess_count(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     seed\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed,\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m/storage/qiguojunLab/caojinjin/codes/openpi/src/openpi/training/data_loader.py:92\u001b[0m, in \u001b[0;36mcreate_dataset\u001b[0;34m(data_config, model_config)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FakeDataset(model_config, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m dataset_meta \u001b[38;5;241m=\u001b[39m \u001b[43mlerobot_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLeRobotDatasetMetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m lerobot_dataset\u001b[38;5;241m.\u001b[39mLeRobotDataset(\n\u001b[1;32m     94\u001b[0m     data_config\u001b[38;5;241m.\u001b[39mrepo_id,\n\u001b[1;32m     95\u001b[0m     delta_timestamps\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mdata_config\u001b[38;5;241m.\u001b[39mlocal_files_only,\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_config\u001b[38;5;241m.\u001b[39mprompt_from_task:\n",
      "File \u001b[0;32m/storage/qiguojunLab/caojinjin/codes/openpi/.venv/lib/python3.11/site-packages/lerobot/common/datasets/lerobot_dataset.py:88\u001b[0m, in \u001b[0;36mLeRobotDatasetMetadata.__init__\u001b[0;34m(self, repo_id, root, local_files_only)\u001b[0m\n\u001b[1;32m     86\u001b[0m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpull_from_repo(allow_patterns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[43mload_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats \u001b[38;5;241m=\u001b[39m load_stats(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks \u001b[38;5;241m=\u001b[39m load_tasks(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n",
      "File \u001b[0;32m/storage/qiguojunLab/caojinjin/codes/openpi/.venv/lib/python3.11/site-packages/lerobot/common/datasets/utils.py:157\u001b[0m, in \u001b[0;36mload_info\u001b[0;34m(local_dir)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_info\u001b[39m(local_dir: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mINFO_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ft \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    159\u001b[0m         ft[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ft[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/storage/qiguojunLab/caojinjin/codes/openpi/.venv/lib/python3.11/site-packages/lerobot/common/datasets/utils.py:129\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(fpath: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage/qiguojunLab/qiguojun/.cache/huggingface/lerobot/lerobot/aloha_sim_transfer_cube_human/meta/info.json'"
     ]
    }
   ],
   "source": [
    "# Reduce the batch size to reduce memory usage.\n",
    "config = dataclasses.replace(config, batch_size=2)\n",
    "\n",
    "# Load a single batch of data. This is the same data that will be used during training.\n",
    "# NOTE: In order to make this example self-contained, we are skipping the normalization step\n",
    "# since it requires the normalization statistics to be generated using `compute_norm_stats`.\n",
    "loader = _data_loader.create_data_loader(config, num_batches=1, skip_norm_stats=True)\n",
    "obs, act = next(iter(loader))\n",
    "\n",
    "# Sample actions from the model.\n",
    "loss = model.compute_loss(key, obs, act)\n",
    "\n",
    "# Delete the model to free up memory.\n",
    "del model\n",
    "\n",
    "print(\"Loss shape:\", loss.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
